{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project - Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analytics and predictive modelling on data from *Food.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are performing exploratory data analytics and predictive modelling to solve some business needs we identified in Food.com and also solutions that are helpful to their customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using data from the following kaggle project: https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food.com** is a place where you can find recipies for all ocassions. It is a social networking platform for people who like to try new recipes and people who like to make new recipes. \n",
    "\n",
    "The website has a lot of features that attract people and retain them. There are sections where you can find ratings and reviews for the recipes which makes it perferct for people to double-check that is the recipe they want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from kaggle website has Recipes, Interactions and User information. We are only considering Recipes and Interactions for our analysis. Interations being the reviews and ratings posted for each recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN BELOW & CLICK THE BUTTON  TO TOGGLE  CODE \n",
    "##### This is because we have a big piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "javascript_functions = {False: \"hide()\", True: \"show()\"}\n",
    "button_descriptions  = {False: \"Show code\", True: \"Hide code\"}\n",
    "\n",
    "\n",
    "def toggle_code(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Toggles the JavaScript show()/hide() function on the div.input element.\n",
    "    \"\"\"\n",
    "\n",
    "    output_string = \"<script>$(\\\"div.input\\\").{}</script>\"\n",
    "    output_args   = (javascript_functions[state],)\n",
    "    output        = output_string.format(*output_args)\n",
    "\n",
    "    display(HTML(output))\n",
    "\n",
    "\n",
    "def button_action(value):\n",
    "\n",
    "    \"\"\"\n",
    "    Calls the toggle_code function and updates the button description.\n",
    "    \"\"\"\n",
    "\n",
    "    state = value.new\n",
    "\n",
    "    toggle_code(state)\n",
    "\n",
    "    value.owner.description = button_descriptions[state]\n",
    "\n",
    "\n",
    "state = False\n",
    "toggle_code(state)\n",
    "\n",
    "button = widgets.ToggleButton(state, description = button_descriptions[state])\n",
    "button.observe(button_action, \"value\")\n",
    "\n",
    "display(button)\n",
    "\n",
    "print(\"Refernces :https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 2)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "### sklearn Pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "### sklearn Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "### sklearn models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "### Others\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from subprocess import check_output\n",
    "from apyori import apriori\n",
    "import warnings\n",
    "\n",
    "#!pip install jupyter_contrib_nbextensions && jupyter contrib nbextension install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the recipes data which is in the csv format directly into a dataframe and explore it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipeDataUrl = 'RAW_recipes.csv'\n",
    "rawData = pd.read_csv(recipeDataUrl,low_memory=False)\n",
    "recipeColumns = rawData.columns\n",
    "print(recipeColumns)\n",
    "print(\"Number of columns: \",len(recipeColumns))\n",
    "rawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes **id** and **contributor_id** are clearly identifiers, so let's convert them into string objects.\n",
    "\n",
    "Also let's set the **recipe id** as the index for each row in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData = rawData.astype({'id': 'object','contributor_id': 'object'})\n",
    "rawData['id_copy'] = rawData['id']\n",
    "rawData1 = rawData.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of total recipes: \",rawData[\"id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of contributors: \",rawData[\"contributor_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe the numerical fields in the data and look at their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviewDataUrl = 'RAW_interactions.csv'\n",
    "reviewData = pd.read_csv(reviewDataUrl,low_memory=False)\n",
    "reviewColumns = reviewData.columns\n",
    "print(reviewColumns)\n",
    "print(\"Number of columns: \",len(reviewColumns))\n",
    "reviewData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of reviews: \",reviewData[\"recipe_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of contributors: \",reviewData[\"user_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize the interactions data based on recipe_id, so that we might have the mean rating for each recipe and also the number of reviews posted for each recipe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rating = reviewData[['recipe_id','rating']].groupby('recipe_id',as_index=False)['rating']\\\n",
    "                    .agg(['mean','count'])\\\n",
    "                    .rename(columns={\"mean\": \"mean_rating\", \"count\": \"review_count\"})\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Interations data with the original recipe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData2 = rawData1.join(rating)\n",
    "print(rawData2.columns)\n",
    "rawData2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in it’s original format has features like nutritional values, ingredients, steps as lists and because of reading in from the CSV format, the lists are read and understood as strings by pandas rather than a python list object!\n",
    "\n",
    "Let's convert the necessary fields to a more usable formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting ingredients to usable strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_list(x):\n",
    "    strlistF = []\n",
    "    strlist = eval(x)\n",
    "    for item in strlist:\n",
    "        if len(item) > 2 and not re.search(\"[^a-zA-Z\\s]\",item):\n",
    "            temp1 = item.strip()\n",
    "            temp2 = temp1.replace(\" \",\"_\")\n",
    "            strlistF.append(temp2)\n",
    "\n",
    "    return(\" \".join(strlistF))\n",
    "\n",
    "\n",
    "rawData2['ingr_str'] = rawData2['ingredients'].apply(get_list)\n",
    "rawData2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the nutritional values to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allNutriList=['cal', 'totalFat', 'sugar', 'sodium', 'protein', 'satFat', 'carbs']\n",
    "\n",
    "recpNutr = pd.DataFrame(rawData1['nutrition'].apply(eval).to_list(),\\\n",
    "                        index=rawData1.index\\\n",
    "                        ,columns=allNutriList)\n",
    "\n",
    "rawData3 = rawData2.join(recpNutr)\n",
    "RAW_recipes = rawData3.copy()\n",
    "print(RAW_recipes.columns)\n",
    "RAW_recipes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1,figsize=(10,6))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "cmap_list = plt.get_cmap('viridis').colors\n",
    "\n",
    "n, bins, patches = ax.hist(rawData3['mean_rating'],alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=12\n",
    "\n",
    "ax.set(xlabel='Average Rating', ylabel='Frequency', title=\"Average Rating histogram\", yscale=\"log\")\n",
    "plt.savefig('Rating histogram.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1,figsize=(10,6))\n",
    "plt.style.use('seaborn-deep')\n",
    "cmap_list = plt.get_cmap('cividis').colors\n",
    "\n",
    "n, bins, patches = ax.hist(rawData3['review_count'],alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=25\n",
    "\n",
    "ax.set(xlabel='Number of Reviews', ylabel='Frequency', title=\"Review Frequency histogram\", yscale=\"log\")\n",
    "plt.savefig('Review histogram.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData3.sort_values(\"review_count\",ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = ['minutes', 'n_steps', 'n_ingredients', 'cal', 'totalFat', 'sugar', 'sodium', 'protein', 'satFat', 'carbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawData3 = rawData3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, axis = plt.subplots(2,5,figsize=(14,10))\n",
    "axis = axis.ravel()\n",
    "colors = plt.get_cmap('Set1',15).colors\n",
    "for i,ax in enumerate(axis):\n",
    "    sns.boxplot(data=rawData3[numeric_cols[i]],color=colors[i+1],ax=ax)\n",
    "    ax.set(title=numeric_cols[i])\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxplots represent the distributions of the numeric features in our data. In all of the features there are few extreme values that are completely skewing the distributions. Such values can be called as outliers.\n",
    "\n",
    "We will need to handle these outliers before moving forward with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing clamping technique to remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see outliers in above box plot. But how is the boundary for the outlier is decided (the two horizontal lines which we see before the outliers)? So those values are decided by the the Inter Quartile Range (IQR) which is differenec of first and third quartile. \n",
    "So using that I can set my lower and upper bound as :\n",
    "lower bound = Q1 - 1.5 * IQR\n",
    "upper bound = Q3 + 1.5 * IQR\n",
    "\n",
    "Tha values out of these range is considered as outliers and we can remove them. So let's do the same for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing values according to IQR\n",
    "Col_to_clamp=['n_steps','minutes','n_ingredients','cal', 'totalFat', 'sugar',   'sodium', 'protein', 'satFat', 'carbs']\n",
    "for col in Col_to_clamp:\n",
    "    IQR = RAW_recipes[col].quantile(0.75) - RAW_recipes[col].quantile(0.25)\n",
    "    colmax = RAW_recipes[col].quantile(0.75) + 2.5 * IQR\n",
    "    colmin = RAW_recipes[col].quantile(0.25) - 2.5 * IQR\n",
    "    RAW_recipes = RAW_recipes[ (RAW_recipes[col] < colmax) & (RAW_recipes[col] > colmin) ]\n",
    "# RAW_recipes=RAW_recipes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RAW_recipes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The number of recipes remaining after handling outliers:\", RAW_recipes['name'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at distributions of some features before and after handling outliers through histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minutes feature before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1)\n",
    "plt.style.use('seaborn-deep')\n",
    "cmap_list = plt.get_cmap('plasma').colors\n",
    "\n",
    "n, bins, patches = ax.hist(rawData['minutes'],alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=6\n",
    "\n",
    "ax.set(xlabel='Minutes', ylabel='Frequency',title=\"Histogram for Minutes before Pre-processing\",yscale=\"log\")\n",
    "plt.savefig('minutes_before.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minutes feature after**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "cmap_list = plt.get_cmap('plasma').colors\n",
    "\n",
    "n, bins, patches = ax.hist(RAW_recipes['minutes'],bins=40,alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=6\n",
    "\n",
    "ax.set(xlabel='Minutes', ylabel='Frequency',title=\"Histogram for Minutes after Pre-processing\")\n",
    "plt.savefig('minutes_after.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_steps feature before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "cmap_list = plt.get_cmap('magma').colors\n",
    "\n",
    "n, bins, patches = ax.hist(rawData['n_steps'],bins=40,alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=6\n",
    "\n",
    "ax.set(xlabel='Number of steps', ylabel='Frequency',title=\"Histogram for Number of steps before Pre-processing\")\n",
    "plt.savefig('steps_before.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_steps feature after**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8,ax = plt.subplots(1)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "cmap_list = plt.get_cmap('magma').colors\n",
    "\n",
    "n, bins, patches = ax.hist(rawData['n_ingredients'],bins=40,alpha=0.7)\n",
    "\n",
    "# apply the same color for each class to match the map\n",
    "idx = 0\n",
    "for c, p in zip(bins, patches):\n",
    "    plt.setp(p, 'facecolor', cmap_list[idx])\n",
    "    idx+=6\n",
    "\n",
    "ax.set(xlabel='Number of steps', ylabel='Frequency',title=\"Histogram for Number of steps after Pre-processing\")\n",
    "plt.savefig('steps_after.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see if there are any Null values that we need to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Checking if Null values exist:\")\n",
    "print(RAW_recipes.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlations between all the numerical fields in original data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,figsize=(14,8))\n",
    "sns.heatmap(RAW_recipes[[\"minutes\",\"mean_rating\",\"n_steps\",\"n_ingredients\",\"review_count\"]].corr(), vmin=-1, vmax=1, center=0, annot=True, cmap=plt.get_cmap('RdBu'), square=True, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(),rotation=0)\n",
    "plt.savefig('correlation_heat_map.png', bbox_inches='tight')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no significant correlations between any of these fields, making them very independent of each other. This raises two situations:\n",
    "1. Since there are no correlations, predictive models are more relaible.\n",
    "2. Since there are no correlations, it will be hard to extract insights through relationships between various fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was to our suprise to learn that Food.com doesn't contain the information about a recipe's cuisine. \n",
    "\n",
    "We can try to introduce the recipe information using the basic instincts of **Data Engineering** and the concepts of **Data Mining**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ?\n",
    "We are using the dataset from one of the famous website in its domain, food.com, It provides recipies for thousands of dishes (to be precise: 231637). So basically this website have recipies for every event you can think of such as pool parties, christmas holidays and so on.\n",
    "<p>\n",
    "But it was to our suprise that they dont have any filter for cuisines. Even in their dataset they dont have any field which they can leverage to have this extended feature on their website.\n",
    "<p>\n",
    "\n",
    "Hence we move ahead to fix this problem using **data engineering** basic instincts and the skills we have learnt in the **Data Minning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Data Engineering ?\n",
    "Data engineering is the aspect of data science that focuses on **practical applications of data collection and analysis**. For all the work that data scientists do to answer questions using large sets of information, there have to be mechanisms for **collecting and validating that information**.\n",
    "\n",
    "\n",
    "Ian Buss, principal solutions architect at Cloudera, notes that data scientists focus on finding new insights from a data set, while data engineers are concerned with the production readiness of that data and all that comes with it: **formats, scaling, resilience and security**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So lets start.. \n",
    "\n",
    "First we have analysed the data set and we found that ingridents would be the best field in the exsisting dataset to use and leverage and predict cuisine for every recipie. \n",
    "\n",
    "Then using one similar dataset where we had ingridients and cuisines we trained our model upto the accuracy of ~75%\n",
    "\n",
    "\n",
    "## Major steps and strategy \n",
    "\n",
    "1. We have 3 files in total which are as follows : \n",
    "    * Train.json : this is with ingridients and cuisines \n",
    "    * Test.json : This is with ingridients only\n",
    "    * RAW_recipes.csv : This is the food.com data set in which we intend to add cuisine for each recipie.\n",
    "2. So using Train.csv we split this dataset into test and train<br>\n",
    "3. We apply multiple model and check and get maximum accuracy.(in our case random forest classifier performs best).<br>\n",
    "4. Having done that we can now proceed on the dummy data set Test.csv this is just an extra step that where we are predicting cuisines from the ingridients and checking manually that every thing is working good before we scale our solution to an entire dataset.<br>\n",
    "5. After we have predicted cuisine now its time to predict the cuisines of entire data set. so we run the predict function giving tf-idf matrix for the ingridients.<br>\n",
    "6. Once we have the predictions we can add this column to the main dataframe.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading  CSVs \n",
    "\n",
    "Reading train.json which has all the data bot ingridients and cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "train.head()\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading test.json which has only ingridients. this is our dummy test file to see our model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_json('test.json')\n",
    "test.head()\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_R=rawData2.copy()\n",
    "df_R.head()\n",
    "df_R.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the important part and we must take care since we are dealing with categorical data we need to vectorize our data. For that wew are using TF-IDF.\n",
    "\n",
    "## What is TF-IDF ?\n",
    "Tf-idf is a very common technique for determining roughly what each document in a set of documents is “about”. It cleverly accomplishes this by looking at two simple metrics: tf (term frequency) and idf (inverse document frequency).<br>\n",
    "\n",
    "**Term frequency :** It is the proportion of occurrences of a specific term to total number of terms in a document.\n",
    "\n",
    "**Inverse document frequency :** It is the inverse of the proportion of documents that contain that word/phrase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['ingredient_list'] = [','.join(z).strip() for z in train['ingredients']]\n",
    "\n",
    "ingredients = train['ingredient_list']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix= vectorizer.fit_transform(ingredients).todense()\n",
    "cuisines = train['cuisine']\n",
    "\n",
    "print(\"TF-IDF Matrix looks like below :\\n\",tfidf_matrix,\"\\n\")\n",
    "print(\"Cuisine looks like below :\\n\",cuisines.head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Train\n",
    "\n",
    "Now that we have data ready which can be further used to train our model we will move ahead straight to train our model. The only thing is since we are using Random Forest Classifier we can pass mulitple parameters with different configuration. So in order to get the best suitable model we are using **GRID SEARCH**\n",
    "\n",
    "### What is Grid Search?\n",
    "Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n",
    "\n",
    "You can change these values and experiment more to see which value ranges give better performance. A cross validation process is performed in order to determine the hyper parameter value set which provides the best accuracy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, cuisines, test_size=0.2)\n",
    "param_grid = {'n_estimators': [100]}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are here checking the model score and the best parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"best param\",grid.best_params_)\n",
    "print(\"best score\",grid.best_score_)\n",
    "print(\"best estimator\",grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How accurate is the model ?\n",
    "\n",
    "To answer the above question we are evaluation our model on 3 basic parameters whcih are :\n",
    "* The Score of the model (grid.score)\n",
    "* The Accuracy of the model (accuracy.score)\n",
    "* Classification rate (Using classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"model score : \",grid.score(X_test, y_test))\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"model accuracy : \",accuracy_score(y_test, y_pred))\n",
    "cuisines = train['cuisine'].value_counts().index\n",
    "print(classification_report(y_test, y_pred, target_names=cuisines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have disscussed multiple times earlier our model is ready to be deployed and we can start predicting the cuisine given the ingridents. We just have to make sure that since we trained our model with the TF_IDF vectorizer we must use the same for predictions.\n",
    "\n",
    "Using our dummy test dataset we first convert the ingredients to the vector and then pass it to grid.predict()\n",
    "this will give us the cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['ingredient_list'] = [','.join(z).strip() for z in test['ingredients']]\n",
    "test_ingredients = test['ingredient_list']\n",
    "test_tfidf_matrix = vectorizer.transform(test_ingredients)\n",
    "test_cuisines = grid.predict(test_tfidf_matrix)\n",
    "test['cuisine'] = test_cuisines\n",
    "test.iloc[7:8,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see from the above modified Dataframe that our model is predicting things quite nicely. So we will move on to applying the same model on the entire data set.\n",
    "\n",
    "We just need to keep in mind the same thing that since we trained our model with the TF_IDF vectorizer we must use the same for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_R['ingredient_list'] = [''.join(z).strip() for z in df_R['ingredients']]\n",
    "df_R_ingredients = df_R['ingredient_list']\n",
    "df_R_tfidf_matrix = vectorizer.transform(df_R_ingredients)\n",
    "df_R_cuisines = grid.predict(df_R_tfidf_matrix)\n",
    "df_R['cuisine'] = df_R_cuisines\n",
    "df_R.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph to classify recipe on the basis of the cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_interactions = df_R.groupby('cuisine')['cuisine']\n",
    "df_cusine = pd.concat([gb_interactions.count()],axis=1)\n",
    "df_cusine.rename(columns = {'cuisine':'cuisine_count'}, inplace = True) \n",
    "df_cusine=df_cusine.reset_index()\n",
    "\n",
    "import matplotlib.pyplot \n",
    "fig8,ax = plt.subplots(1,figsize = (13,6))\n",
    "ax.bar(color=sns.color_palette('GnBu_r',21),x=df_cusine['cuisine'],height=df_cusine['cuisine_count'],width=1)\n",
    "ax.set(xlabel='Cuisine', ylabel='No of Recipies',title=\"Recipies per cuisine\")\n",
    "plt.xticks(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score=[]\n",
    "for cusine in df_R['cuisine'].unique():\n",
    "    df_per_cuisine=df_R[df_R['cuisine']==cusine]\n",
    "    average=df_per_cuisine['minutes'].sum()/df_per_cuisine['minutes'].count()\n",
    "    score.append({\"cuisine\":cusine,\"average\":average.round(2)})\n",
    "\n",
    "avg_min_per_cuisine=pd.DataFrame(score)\n",
    "avg_min_per_cuisine = avg_min_per_cuisine.drop(avg_min_per_cuisine[avg_min_per_cuisine.average > 500].index)\n",
    "avg_min_per_cuisine\n",
    "\n",
    "import matplotlib.pyplot \n",
    "fig8,ax = plt.subplots(1,figsize = (13,6))\n",
    "ax.bar(color=sns.color_palette('GnBu_r',21),x=avg_min_per_cuisine['cuisine'],height=avg_min_per_cuisine['average'],width=1)\n",
    "ax.set(xlabel='Cuisine', ylabel='No of Avg Minutes ',title=\"Avgerage Number of minutes per cuisine\")\n",
    "plt.xticks(rotation=65)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score=[]\n",
    "for cusine in df_R['cuisine'].unique():\n",
    "    df_per_cuisine=df_R[df_R['cuisine']==cusine]\n",
    "    average=df_per_cuisine['review_count'].sum()\n",
    "    score.append({\"cuisine\":cusine,\"average\":average.round(2)})\n",
    "\n",
    "avg_min_per_cuisine=pd.DataFrame(score)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot \n",
    "fig8,ax = plt.subplots(1,figsize = (13,6))\n",
    "ax.bar(color=sns.color_palette('GnBu_r',21),x=avg_min_per_cuisine['cuisine'],height=avg_min_per_cuisine['average'],width=1)\n",
    "ax.set(xlabel='Cuisine', ylabel='Review Count ',title=\"Review Count per cuisine\")\n",
    "plt.xticks(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score=[]\n",
    "for cusine in df_R['cuisine'].unique():\n",
    "    df_per_cuisine=df_R[df_R['cuisine']==cusine]\n",
    "    average=df_per_cuisine['mean_rating'].sum()/df_per_cuisine['mean_rating'].count()\n",
    "    score.append({\"cuisine\":cusine,\"average\":average.round(2)})\n",
    "\n",
    "avg_min_per_cuisine=pd.DataFrame(score)\n",
    "avg_min_per_cuisine = avg_min_per_cuisine.drop(avg_min_per_cuisine[avg_min_per_cuisine.average > 500].index)\n",
    "avg_min_per_cuisine\n",
    "\n",
    "import matplotlib.pyplot \n",
    "fig8,ax = plt.subplots(1,figsize = (13,6))\n",
    "ax.bar(color=sns.color_palette('GnBu_r',21),x=avg_min_per_cuisine['cuisine'],height=avg_min_per_cuisine['average'],width=1)\n",
    "ax.set(xlabel='Cuisine', ylabel='Rating ',title=\"Average rating per cuisine\")\n",
    "plt.xticks(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "ser = df_R['cuisine']\n",
    "textt=ser.str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"#E8E8E8\",colormap=\"inferno_r\").generate(textt)\n",
    "plt.figure(figsize=(20,10))\n",
    "# plt.savefig('topCluster.png', format='png')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('topcluster.png', facecolor='k', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* After successfully integrating cuisine into the data, we can see that the Average rating for each cuisine is more or less the same. There's not much of a trend. \n",
    "* The number of ratings per cuisine doesn't specifically give us any information given the high volumes of recipes for specific cuisines.\n",
    "* But what if we want to understand what kind of recipes become more popular based on some other values available in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In such senarios, ***Clustering*** comes very handy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering:**\n",
    "* Clustering is the process of grouping similar data points based on similarity/distance metrics.\n",
    "* It can uncover patterns which are previously undiscovered.\n",
    "* Each group can be assumed as a different class of the data.\n",
    "* While the data points in a cluster are selected to maximize similarity between them, the clusters should be very dissimilar. \n",
    "* Most clustering algorithms deal with noise in the data.\n",
    "* For example, in social networks, users can be clustered based on their likes and dislikes.\n",
    "\n",
    "**For this analysis, we will use K-Means clustering.**\n",
    "\n",
    "\n",
    "**K-Means clustering algorithm:**\n",
    "* K-means algorithm is a partition based clustering algorithm. \n",
    "* **K** here is the number of clusters to be aggregated from the data.\n",
    "* When we define the **K** value, the algorithm finds the very number of means from the data.\n",
    "* It will find distance of each of the data points from the means cluusters them to the nearest mean.\n",
    "* These means are called cluster centers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First let's try to cluster the data by nutrition values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by nutritional values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our nutrition values have differeng ranges and scales of values, it is important we normalize them. We will use the **Normalizer** function from sklearn for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recpData2 = RAW_recipes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking all the nutrition columns and normalizing them\n",
    "scaled = Normalizer().fit_transform(recpData2[allNutriList])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply K-means clustering algorithm on the normalized data.\n",
    "\n",
    "But to find the optimal value for **K**, we will use the elbow plot. Elbow plot shows us the inertia score for each K value. In this plot, the point where the line bends like an elbow of a hand is considered the optimal value for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(scaled)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the above graph we can see that there is slight bend near the value 4, thus 4 can be considered as the value of K.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to visualize the 4 clusters on a 2D graph, we will use PCA for dimensionality reduction.\n",
    "\n",
    "**Principal Component Analysis** is a statistical procedure to perform feature extraction, when we have too many features to work with.\n",
    "* The algorithm is mainly used for reducing features to either limit over fitting the data or to visualize in a 2 dimensional or 3 dimensional plot. \n",
    "* It mainly works on the variation of the features available for a data point and brings out strong underlying structures. \n",
    "* These help us to understand and visualize the data more easily.\n",
    "* PCA itself doesn't remove any features, but computes new features as a function of one or more existing features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(scaled)\n",
    "\n",
    "\n",
    "print(\"The explained variance ratios are :\",pca.explained_variance_ratio_)\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='red')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the explained variance ratios of each Principal component evaluated. Explained variance is the percentage of data explained by a principal component. As we can see, between Principal component 0 and 1, more than 75% of our data is being explained. So we can use the first 2 principal components to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4)\n",
    "model.fit(scaled)\n",
    "colors = plt.get_cmap('Set1',10).colors\n",
    "with plt.style.context('classic'):\n",
    "    plt.scatter(PCA_components[0], PCA_components[1], alpha=0.8, color=colors[model.labels_])\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see clusters are very clearly separated in the data. Let's see some results from our clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of recipes in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recpData2['nutr_cluster'] = model.labels_\n",
    "recpData2['nutr_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's summaize the data by the cluster and look at some properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutr_results = recpData2.groupby(['nutr_cluster']).mean().sort_values(\"mean_rating\",ascending=True)\n",
    "nutr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are no properties that define these clusters appropriately. Thus, clustering through Nutritional values didn't give us any good insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There is one more important field that defines a recipe, ingredients. Ingredients used in a recipe define both the nutritional values and cuisine of the item, thus playing an important role. We will now attempt to cluster based on ingredients to exploratorily search for insights.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing through TF-IDF vectorizer has already been explained in the previous section and we will be using the same preprocessing even for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing by spliting by space\n",
    "def word_splitter(in_string):\n",
    "    tokens = in_string.split()\n",
    "    return tokens\n",
    "\n",
    "#Creating Vectorizer model, with taking top 500 words from the description\n",
    "vectorize = TfidfVectorizer(max_features=500, tokenizer=word_splitter, stop_words='english')\n",
    "\n",
    "ingr_matrix = vectorize.fit_transform(recpData2['ingr_str'])\n",
    "\n",
    "#Converting the spared matrix to dense matrix and creating pandas DF from it\n",
    "ingr_DF = pd.DataFrame(ingr_matrix.todense(),index=recpData2.index)\n",
    "ingr_DF.columns=vectorize.get_feature_names()\n",
    "ingr_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the elbow plot to find the optimal **K**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#Passing the vectorized matrix created above to k-Means model for clustering\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, random_state=0)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(ingr_DF)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ks, inertias, '-o', color='#FC4E4E')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.savefig('inertia.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "end = time.time()\n",
    "print(\"Process completed - %s seconds elapsed.\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we cannot discern a **K** value easily. Even though the graph is not straightforward, we have reasonable doubt between values 4,5 & 6.\n",
    "\n",
    "In this case, we will need another method to find the optimal **K** from the values 4,5 & 6. \n",
    "\n",
    "**We will use the silhouette score.**\n",
    "* Silhouette method measures how similar a point is to it’s own cluster compared to others.\n",
    "* It is more likely a validation rather than a decision maker. Which is exactly what we want in this scenario.\n",
    "* By using Euclidean distance as the metric, we will plot the graph for silhouette scores for the three values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#Passing the vectorized matrix created above to k-Means model for clustering\n",
    "ks = range(4, 7)\n",
    "sil = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, random_state=0)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(ingr_DF)\n",
    "    labels = model.labels_\n",
    "    sil.append(silhouette_score(ingr_DF, labels, metric = 'euclidean'))\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ks, sil, '-o', color='#FC4E4E')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.xticks(ks)\n",
    "plt.savefig('silh.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "end = time.time()\n",
    "print(\"Process completed - %s seconds elapsed.\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, we can confidently say that the **5** is the most optimal value for **K**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5, random_state=0)\n",
    "model.fit(ingr_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the number of recipes in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recpData2['ingr_cluster'] = model.labels_\n",
    "recpData2['ingr_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the top-ingredients in each of our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nCrucial ingredients for each clusters: \\n')\n",
    "features = vectorize.get_feature_names()\n",
    "centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "for clust in range(0,5):\n",
    "    print('Cluster '+str(clust)+': ')\n",
    "    for ind in centroids[clust, :15]:\n",
    "        print(features[ind]+' ', end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing the data on Cluster number to look at some properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingr_results = recpData2.groupby(['ingr_cluster']).mean().sort_values(\"mean_rating\",ascending=True)\n",
    "ingr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 features that define our clusters very well in terms of popularity. \n",
    "\n",
    "* As we can see the cluster that has the highes average rating has the lowest sugar values and the highest protein values and\n",
    "* the cluster with the least average rating has the highest sugar values and the least protein values.\n",
    "\n",
    "> Thus it shows us that recipes that contain less sugars and more proteins are more prone to beome popular than the sugary sweets and cakes with less protein value. \n",
    "\n",
    "*People on Food.com turned out to be healthy makers or eaters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please see below for bar-plots showing the sugar and protein levels of each cluster. Darker the color of bar, higher it's average rating.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig,ax = plt.subplots(1,figsize=(6,3))\n",
    "ax.bar(ingr_results.index,ingr_results.sugar,color=sns.color_palette(\"YlGn\",5))\n",
    "ax.set(title=\"Sugar levels\",xlabel=\"Clusters\",ylabel=\"sugar\")\n",
    "plt.savefig('sugar.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,figsize=(6,3))\n",
    "ax.bar(ingr_results.index,ingr_results.protein,color=sns.color_palette(\"Blues\",5))\n",
    "ax.set(title=\"Protein levels\",xlabel=\"Clusters\",ylabel=\"Protein\")\n",
    "plt.savefig('protein.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's make word clouds for our most popular and least popular clusters and look at the ingredients precedence in them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "ser = pd.Series(recpData2[recpData2[\"ingr_cluster\"]==4][\"ingr_str\"].apply(lambda x: x.replace(\"_\",\" \")))\n",
    "textt=ser.str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"#E8E8E8\",colormap=\"inferno_r\").generate(textt)\n",
    "plt.figure(figsize=(20,10))\n",
    "# plt.savefig('topCluster.png', format='png')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('topcluster.png', facecolor='k', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "ser = pd.Series(recpData2[recpData2[\"ingr_cluster\"]==2][\"ingr_str\"].apply(lambda x: x.replace(\"_\",\" \")))\n",
    "textt=ser.str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color=\"#323232\",colormap=\"gist_rainbow\").generate(textt)\n",
    "plt.figure(figsize=(20,10))\n",
    "# plt.savefig('topCluster.png', format='png')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('last.png', facecolor='k', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After classification and clustering, now we will see regression analysis. As we know regression analysis is done when we want to predict continious value.\n",
    "In our dataset we have nutritional values as 'cal', 'totalFat', 'sugar', 'sodium', 'protein', 'satFat', 'carbs' for each receipes. But we saw that for many of the receipes the nutritional values except calories and carbs are zero/missing.\n",
    "Hence we created a model using Gradient Boosting to predict the 'totalFat', 'sugar',   'sodium', 'protein', 'satFat' using values of 'cal','carbs'\n",
    "\n",
    "On the similar lines are also predicting number of ingredients which will be used in a dish using the number of steps and time required to make the dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "testdf4totalFat=pd.DataFrame()\n",
    "def mean_squared_error_(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions) ** 0.5\n",
    "\n",
    "def GradientBoostingRegressor_function(X_train, y_train,X_test):\n",
    "    gbrt=GradientBoostingRegressor(n_estimators=12, learning_rate=1, max_depth=2, random_state=0) \n",
    "    gbrt.fit(X_train, y_train) \n",
    "    return(gbrt.predict(X_test))\n",
    "\n",
    "def r2scoreFun(y_test, y_pred):\n",
    "    r2score=r2_score(y_test, y_pred)\n",
    "    return r2score\n",
    "    \n",
    "def toPredict(col_predictors,col_to_predict):\n",
    "    predictedDF=pd.DataFrame(columns=['Predicted','with error','R2 Score'])\n",
    "    for i,col in enumerate(col_to_predict):\n",
    "        X=RAW_recipes[col_predictors]\n",
    "        y=RAW_recipes[[col]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)\n",
    "        y_pred=GradientBoostingRegressor_function(X_train, y_train,X_test)\n",
    "        y_test=y_test.reset_index(drop=True)\n",
    "        RMSE=mean_squared_error_(y_test, y_pred)\n",
    "        r2score=r2scoreFun(y_test, y_pred)\n",
    "        predictedDF.loc[i]=[col,RMSE,r2score]\n",
    "        \n",
    "        if(col=='totalFat'):\n",
    "            testdf4totalFat[col+'_y_pred']=y_pred\n",
    "            testdf4totalFat[col+'_y_test']=y_test\n",
    "            testdf4totalFat[col+'_abs_diff']=testdf4totalFat.apply(lambda x : abs(x[col+'_y_pred']-x[col+'_y_test']), axis=1)\n",
    "        \n",
    "        testdf=pd.DataFrame()\n",
    "        testdf[col+'_y_pred']=y_pred\n",
    "        testdf[col+'_y_test']=y_test\n",
    "        testdf[col+'_abs_diff']=testdf.apply(lambda x : abs(x[col+'_y_pred']-x[col+'_y_test']), axis=1)\n",
    "        print()\n",
    "        print(\"Some of the predicted values:\")\n",
    "        print(testdf.sort_values(by=[col+'_abs_diff']).reset_index(drop=True).head(5))\n",
    "        print()\n",
    "    print(\"\\n\\n-----Predicting using \"+str(col_predictors)+'-----\\n')\n",
    "    print(predictedDF.sort_values(by=['with error']).reset_index(drop=True))\n",
    "    return predictedDF\n",
    "    \n",
    "col_predictors_nutri=['cal','carbs']\n",
    "col_to_predict_nutri=['totalFat', 'sugar',   'sodium', 'protein', 'satFat']\n",
    "Nutri_predictedDF=toPredict(col_predictors_nutri,col_to_predict_nutri)\n",
    "print(\"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "col_predictors_ingre=['n_steps','minutes']\n",
    "col_to_predict_ingre=['n_ingredients']\n",
    "cont_predictedDF=toPredict(col_predictors_ingre,col_to_predict_ingre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are predicting continuous value we can not measure accuracy but we can calculate the distance of actual and predicted value, which will be the error. One of the measure of error is Root Mean Square Error (RMSE). \n",
    "\n",
    "So let's see what is the RMSE for different nutritional values predicted from Calories and Carbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nutri_predictedDF=Nutri_predictedDF.sort_values(by=['with error']).reset_index(drop=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.bar(Nutri_predictedDF['Predicted'], Nutri_predictedDF['with error'], align='center', alpha=0.5,color='red')\n",
    "plt.xlabel('Nutritional Values')\n",
    "plt.ylabel('RMS Error')\n",
    "plt.title('Error in predicting other Nutritional Values using Calories and Carbs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see we are able to predict TotalFat with least error and Sugar with highest error. This means we are able to predict TotalFat with most accuracy and Sugar with least accuracy.\n",
    "\n",
    "Further let's see the R-squared (R2) Score, which represents how good the regression line fits the data. So below graph represents R2 Score for predicting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nutri_predictedDF=Nutri_predictedDF.sort_values(by=['R2 Score'],ascending=False).reset_index(drop=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.bar(Nutri_predictedDF['Predicted'], Nutri_predictedDF['R2 Score'], align='center', alpha=0.5,color='green')\n",
    "plt.xlabel('Nutritional Values')\n",
    "plt.ylabel('RMS Error')\n",
    "plt.title('R2 Score in predicting other Nutritional Values using Calories and Carbs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vizulize how our actual and predicted values differs, let plot these values for smaple data.\n",
    "So we are taking 200 actual and predicted values for TotalFat to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testdf4totalFat2h=testdf4totalFat.head(200)\n",
    "testdf4totalFat2h=testdf4totalFat2h.sort_values(by='totalFat_y_test')\n",
    "testdf4totalFat2h['rrange']=range(len(testdf4totalFat2h))\n",
    "\n",
    "fig = plt.figure(figsize=(17, 8))\n",
    "plt.xlabel('Recepies dummy IDs')\n",
    "plt.ylabel('Total_Fat value')\n",
    "plt.title('Predicting Total_Fat from Calories and Carbs')\n",
    "ax = plt.axes()\n",
    "\n",
    "x = testdf4totalFat2h['rrange']\n",
    "y = testdf4totalFat2h['totalFat_y_test']\n",
    "z = testdf4totalFat2h['totalFat_y_pred']\n",
    "plt.plot(x, y,label='Actual Total_Fat')\n",
    "plt.plot(x, z,label='Predicted Total_Fat')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='best', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As initially informed Food.com provides option to buy ingredients for a receipe in their portal. If we consider ingredients of a receipe are bought together, then we can consider them as items of a order/transaction. Some what like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO sample/filter with cusine OR any other criteria\n",
    "RAW_recipes=RAW_recipes.head(1000)\n",
    "ingredients_list = RAW_recipes['ingredients'].tolist()\n",
    "RAW_recipes[['name','ingredients']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess what analysis we can do here to increase the items sale?\n",
    "\n",
    "We can do Market Basket Analysis, which analyzes which items are frequently bought together and hence suggest items to buy based on the items on cart. By implementing this, user can get suggestion more items to add based on what s/he is buying at present.\n",
    "\n",
    "We are using Apriori algorithm to implement Market Basket Analysis. \n",
    "So first like below are creating list of items/ingredients bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RAW_recipes_i=RAW_recipes.reset_index()\n",
    "def Convert_nutri(string): \n",
    "    li = list(string.split('\\', \\'')) \n",
    "    return li\n",
    "allIngredList=[]\n",
    "for ind in range(len(RAW_recipes)):\n",
    "    ss=RAW_recipes_i.loc[ ind , 'ingredients' ]\n",
    "    ss=ss[2:-2]\n",
    "    allIngredList.append(Convert_nutri(ss))\n",
    "records=allIngredList\n",
    "print(\"Showing first to list of ingredients:\")\n",
    "records[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the below results once this list is passed to apriori model with the desired values for parameters of Support, Confidence and Lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO tune apriori parameters\n",
    "associationRules = apriori(records, min_support=0.0050, min_confidence=0.6, min_lift=3, min_length=5,max_length=None)\n",
    "associationResult = list(associationRules)\n",
    "\n",
    "print(\"\\n\\nNumber of Rules:\")\n",
    "print(len(associationResult))\n",
    "\n",
    "print(\"\\n\\nExample of a rule:\")\n",
    "print(associationResult[0])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "associationResult10=associationResult[0:10]\n",
    "\n",
    "print(\"Listing 10 of the rules:\\n\")\n",
    "for item in associationResult10:\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    print(\"Rule: \" + items[0] + \" --> \" + items[1])\n",
    "    print(\"Support: \" + str(item[1]))\n",
    "    print(\"Confidence: \" + str(item[2][0][2]))\n",
    "    print(\"Lift: \" + str(item[2][0][3]))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the blog, let's see what all we did. We started with selecting interesting data. We choose data of Food.com from kaggle. To understand the data first we did the data analysis, where we saw different data files and there length, central tendency metric for various columns/features, the relation between the features and the outlier analysis. Once we analyzed the data and saw the issues, we worked upon to resolve them by handling outliers. \n",
    "\n",
    "Then we started with classification where we introduced the new column as Cusine for our data set. Using the new column we analysed our data and came up with interesting analysis. Second, we performed clustering over the ingredients, for that we performed PCA, vectorization and K-means. Analysing cluster over rating and review provides us with meaningful insights. After that, we saw regression where by help of Carbs and Calories we predected other nutritional values using Gradient Boosting Regression. Last but not the least we performed Market Basket Analysis which can be profitable for the sales.\n",
    "\n",
    "We hope this blog will be helpful, for any suggestions please email at any of the following: \n",
    "agupta33@student.gsu.edu\n",
    "sdasari3@student.gsu.edu\n",
    "sdawani1@student.gsu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://www.ultravioletanalytics.com/blog/tf-idf-basics-with-pandas-scikit-learn\n",
    "* https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998\n",
    "* https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
    "* https://www.kaggle.com/etsc9287/food-com-eda-and-text-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
